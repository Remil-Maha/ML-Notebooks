{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsKv1a/6mkY9Cbb8cueIcL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Remil-Maha/ML-Notebooks/blob/main/FakeNewsDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWovkAtyO_7G",
        "outputId": "dd37a6b7-279f-433b-f60c-12e2787d2adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA"
      ],
      "metadata": {
        "id": "HVtXVfDOSDhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n"
      ],
      "metadata": {
        "id": "w6iZUr-cSCzu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "fcb38cf3-2661-4ba4-8fc5-692fef19ae39",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 3993",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cd75a77447fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 3993"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "\"\"\"\n",
        "id: unique id for a news article\n",
        "title: the title of a news article\n",
        "author: author of the news article\n",
        "text: the text of the article; could be incomplete\n",
        "label: a label that marks the article as potentially unreliable\n",
        "1: unreliable\n",
        "0: reliable\"\"\""
      ],
      "metadata": {
        "id": "VozlhKnWSY2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "wRjUdfmatRbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "S-Zn0vvISaEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "rv6ZI8tsTM3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "_uGnnn75TUB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['author'], inplace=True)"
      ],
      "metadata": {
        "id": "ew7PI9SOWJzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "hv2mBtX_s2RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRUE VS.FAKE DISTRIBUTION**"
      ],
      "metadata": {
        "id": "53XVKOnytigj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category = df['label'].value_counts()\n",
        "print(category)"
      ],
      "metadata": {
        "id": "bGpCQTI4tNf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the Pie Chart:\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(category,labels=category.index , autopct='%1.0f%%')\n",
        "plt.title('Distribution of FAKE vs.TRUE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L_O1eORBtxwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On remarque que les classe sont presque equilibrées"
      ],
      "metadata": {
        "id": "RstbvfG-uIX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "n_9G-iNbvnKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove majuscules  :\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['text']= df['text'].str.lower()"
      ],
      "metadata": {
        "id": "oSab6Y3BvoXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove html tags\n",
        "def remove_html_tags(text):\n",
        "  soup = BeautifulSoup(text,'html.parser')\n",
        "  return soup.get_text()\n",
        "df['title'] =df['title'].apply(remove_html_tags)\n",
        "df['text']  =df['text'].apply(remove_html_tags)"
      ],
      "metadata": {
        "id": "snqhjuQIwV-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wordopt(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]','',text)\n",
        "    # The key is to remove the \"(Reuters)\" string\n",
        "    # The Model during the training part can memorize it and perfrom great in training and badly when other testing input is given.\n",
        "    text = re.sub('[()]','',text)\n",
        "    text = re.sub('\\\\W',' ',text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "df['text'] = df['text'].apply(wordopt)\n",
        "df['title'] = df['title'].apply(wordopt)"
      ],
      "metadata": {
        "id": "9vG3Fds4xZ5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "# Get English stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stop words from text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['title'] = df['title'].apply(remove_stopwords)\n",
        "df['text'] = df['text'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "CgiXyq-yzpjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "J9ECDEsB08oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "df['title'] = df['title'].apply(lambda x : ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n",
        "df['text'] = df['text'].apply(lambda x : ' '.join([porter_stemmer.stem(word) for word in x.split()]))"
      ],
      "metadata": {
        "id": "avFzJzN80hnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "24XQQy_57610"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "sup0JbhF3vlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Vectorization**"
      ],
      "metadata": {
        "id": "K9k0lqBH37OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer()\n"
      ],
      "metadata": {
        "id": "3kddbUuM3usK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['combined_text'] = df['title'] + \" \" + df['text']\n",
        "X = df['combined_text']\n",
        "Y= df['label']"
      ],
      "metadata": {
        "id": "VmgdZf1_8hvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed = vectorizer.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "WrJzGP9-Abk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_transformed)\n",
        "#(document_index, feature_index)    valeur_TFIDF"
      ],
      "metadata": {
        "id": "QSKYsNaw9Y5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression(C=0.1 ,solver='liblinear')\n",
        "lr_model.fit(X_transformed,Y)"
      ],
      "metadata": {
        "id": "bbsBSEV79bhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Test Data:"
      ],
      "metadata": {
        "id": "NvKv4kcIBzla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_X=  pd.read_csv('test.csv',encoding='latin-1')\n",
        "df_test_y = pd.read_csv('submit.csv')\n",
        "df_test = pd.concat([df_test_X, df_test_y], axis=1)\n"
      ],
      "metadata": {
        "id": "9_thkn0OBkXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "BXJ35pexIwWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isnull().sum()"
      ],
      "metadata": {
        "id": "Fn3xEYy7Br57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "id": "op0QxGN6B-UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.drop(columns=['author'], inplace=True)"
      ],
      "metadata": {
        "id": "0qVGLMyuCBI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.duplicated().sum()"
      ],
      "metadata": {
        "id": "jWfER5CoCJ2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "Nxu0qjmRCMG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isnull().sum()"
      ],
      "metadata": {
        "id": "UNzxQE0oCXpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.drop(columns=['id'],inplace=True)"
      ],
      "metadata": {
        "id": "W-nGTmSnJ_Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['title'] = df_test['title'].str.lower()\n",
        "df_test['text']= df_test['text'].str.lower()"
      ],
      "metadata": {
        "id": "-2MwgaXjCYec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['title'] =df_test['title'].apply(remove_html_tags)\n",
        "df_test['text']  =df_test['text'].apply(remove_html_tags)"
      ],
      "metadata": {
        "id": "gjMO8lb-Cvwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['text'] = df_test['text'].apply(wordopt)\n",
        "df_test['title'] = df_test['title'].apply(wordopt)"
      ],
      "metadata": {
        "id": "nKFaeFhSC_HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['title'] = df_test['title'].apply(remove_stopwords)\n",
        "df_test['text'] = df_test['text'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "cvC8yzYUDHf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['title'] = df_test['title'].apply(lambda x : ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n",
        "df_test['text'] = df_test['text'].apply(lambda x : ' '.join([porter_stemmer.stem(word) for word in x.split()]))"
      ],
      "metadata": {
        "id": "MJaR1DXuDUq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "Dl9ZvZtvD4x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['combined_text'] = df_test['title'] + \" \" + df_test['text']\n",
        "X_test = df_test['combined_text']"
      ],
      "metadata": {
        "id": "Rszt_cSEDjdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed_test = vectorizer.transform(X_test)\n",
        "lr_pred = lr_model.predict(X_transformed_test)\n"
      ],
      "metadata": {
        "id": "08fjt6y3Enwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test =df_test['label']"
      ],
      "metadata": {
        "id": "BUWkopnTKgCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "lr_precision = precision_score(y_test, lr_pred, average='weighted')\n",
        "lr_recall = recall_score(y_test, lr_pred, average='weighted')\n",
        "lr_conf_matrix = confusion_matrix(y_test, lr_pred)\n",
        "print(\"Accuracy:\", lr_accuracy)\n",
        "print(\"Precision:\", lr_precision)\n",
        "print(\"Recall:\", lr_recall)\n",
        "print(\"Confusion Matrix:\\n\", lr_conf_matrix)\n"
      ],
      "metadata": {
        "id": "5cAdifFCHySL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}